{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture_07_Document_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8rVH_wrVFN",
        "colab_type": "text"
      },
      "source": [
        "# **Global Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GumGWC6reG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/gdrive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2019-20-BDC/datasets\"\n",
        "DATASET_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/all-the-news-1.csv.bz2\"\n",
        "GDRIVE_DATASET_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_URL.split(\"/\")[-1]\n",
        "\n",
        "RANDOM_SEED = 42 # for reproducibility\n",
        "MAX_K_CLUSTERS = 20 # max number of clusters (more on this later...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv",
        "colab_type": "text"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT",
        "colab_type": "text"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vtwpPGKn9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa",
        "colab_type": "text"
      },
      "source": [
        "## **2.** Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8zPg5APmYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB",
        "colab_type": "text"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQOJijlEz3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV1RghvXuwQG",
        "colab_type": "text"
      },
      "source": [
        "## **4.** Create <code>ngrok</code> tunnel to check the Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB4QVvEQuIXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0pzpMiOA_L",
        "colab_type": "text"
      },
      "source": [
        "## **5.** Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzUd3JdGS1Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6",
        "colab_type": "text"
      },
      "source": [
        "## **6.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqJ5f0JE3BL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhTN342EEOYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu77QbD2vC_o",
        "colab_type": "text"
      },
      "source": [
        "# **Data Acquisition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fI3wiGvKBl",
        "colab_type": "text"
      },
      "source": [
        "Download dataset file from URL directly to our Google Drive.\n",
        "\n",
        "**NOTE:** This is just a sample of the full <code>all-the-news</code> dataset available from [Kaggle](https://www.kaggle.com/snapcrack/all-the-news); more specifically, it is one of the three files which the dataset is composed of (i.e., <code>articles1.csv</code>)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrLDE_4e7KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiuGbfyv8vT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_URL))\n",
        "get_data(DATASET_URL, GDRIVE_DATASET_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_FILE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI",
        "colab_type": "text"
      },
      "source": [
        "### Read dataset file into a Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi5Hd60FFcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df = spark.read.load(GDRIVE_DATASET_FILE, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTd8ep9x74H",
        "colab_type": "text"
      },
      "source": [
        "### Check the shape of the loaded dataset, i.e., number of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRyYqeXGA4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(news_df.count(), len(news_df.columns)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WC4RPQgyEsB",
        "colab_type": "text"
      },
      "source": [
        "### Print out the schema of the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KhtSnvGIwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pjib1fiylb6",
        "colab_type": "text"
      },
      "source": [
        "### Display the first 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4v-Z92rGXoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_0l5Rqz6lg",
        "colab_type": "text"
      },
      "source": [
        "### Count the number of duplicated news (if any)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgKnTs5qno8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The total number of duplicated news are {:d} out of {:d}\".\n",
        "      format(news_df.count() - news_df.dropDuplicates(['content']).count(), news_df.count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIewdnUy43lH",
        "colab_type": "text"
      },
      "source": [
        "### Display the top-10 most duplicated news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc85D2XDq6Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.groupby([\"content\"]).count().sort(\"count\", ascending=False).show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtcnTISuZlMh",
        "colab_type": "text"
      },
      "source": [
        "### Remove duplicate news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGHCoGwB9mBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df = news_df.dropDuplicates([\"content\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlbuZU7N9vIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The total number of unique news is: {:d}\".format(news_df.count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELIYdrnk04Yu",
        "colab_type": "text"
      },
      "source": [
        "### Check for any missing value (i.e., <code>NULL</code>) along <code>content</code> column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AQt_-7z1EH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.where(col(\"content\").isNull()).count()\n",
        "# Alternatively, using filter:\n",
        "# news_df.filter(news_df.content.isNull()).count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQLfLzcDYWe",
        "colab_type": "text"
      },
      "source": [
        "### Show the corresponding NULL entry/ies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUTcKAUq1gGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.where(col(\"content\").isNull()).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOlnYMzS4s3w",
        "colab_type": "text"
      },
      "source": [
        "### Remove <code>NULL</code> entry/ies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw9w2Ovu1zn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df = news_df.na.drop(subset=[\"content\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66pFGdch-BU_",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjt71kS-ZwY",
        "colab_type": "text"
      },
      "source": [
        "In this example, we are working with text data and our ultimate goal is to cluster news into groups of coherent \"topics\" using one of the clustering algorithms we know (e.g., K-means). This is a specific task of a more general area, which is referred to as _natural language processing_ (NLP).\n",
        "\n",
        "As **preliminary** steps of any NLP task, at least the following pipeline must be executed first:\n",
        "\n",
        "- Text cleaning:\n",
        " - Case normalization (<code>lower</code>) -> convert all text to lower case;\n",
        " - Filter out _leading_ and _trailing_ whitespaces (<code>trim</code>);\n",
        " - Filter out punctuation symbols (<code>regexp_replace</code>);\n",
        " - Filter out any internal extra whitespace resulting from the step above (<code>regexp_replace</code> + <code>trim</code>).\n",
        "- Tokenization (<code>Tokenizer</code>): splitting raw text into a list of individual _tokens_ (i.e., words), typically using whitespace as delimiter \n",
        "- Stopwords removal (<code>StopWordsRemover</code>): removing so-called _stopwords_, namely words that do not contribute to the deeper meaning of the document like \"the\", \"a\", \"me\", etc.\n",
        "- Stemming (<code>SnowballStemmer</code>): reducing each word to its root or base. For example \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prr9bVUydzmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(df, column_name=\"content\"):\n",
        "    \"\"\" \n",
        "    This fucntion takes the raw text data and apply a standard NLP preprocessing pipeline consisting of the following steps:\n",
        "      - Text cleaning\n",
        "      - Tokenization\n",
        "      - Stopwords removal\n",
        "      - Stemming (Snowball stemmer)\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: the input dataframe along with the `cleaned_content` column as the results of the NLP preprocessing pipeline\n",
        "\n",
        "    \"\"\"\n",
        "    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace\n",
        "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "    # Text preprocessing pipeline\n",
        "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
        "\n",
        "    # 1. Text cleaning\n",
        "    print(\"# 1. Text Cleaning\\n\")\n",
        "    # 1.a Case normalization\n",
        "    print(\"1.a Case normalization:\")\n",
        "    lower_case_news_df = df.select(\"id\", lower(col(column_name)).alias(column_name))\n",
        "    lower_case_news_df.show(10)\n",
        "    # 1.b Trimming\n",
        "    print(\"1.b Trimming:\")\n",
        "    trimmed_news_df = lower_case_news_df.select(\"id\", trim(col(column_name)).alias(column_name))\n",
        "    trimmed_news_df.show(10)\n",
        "    # 1.c Filter out punctuation symbols\n",
        "    print(\"1.c Filter out punctuation:\")\n",
        "    no_punct_news_df = trimmed_news_df.select(\"id\", (regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n",
        "    no_punct_news_df.show(10)\n",
        "    # 1.d Filter out any internal extra whitespace\n",
        "    print(\"1.d Filter out extra whitespaces:\")\n",
        "    cleaned_news_df = no_punct_news_df.select(\"id\", trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n",
        "    cleaned_news_df.show(10)\n",
        "\n",
        "    # 2. Tokenization (split text into tokens)\n",
        "    print(\"# 2. Tokenization:\")\n",
        "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
        "    tokens_df = tokenizer.transform(cleaned_news_df).cache()\n",
        "    tokens_df.show(10)\n",
        "\n",
        "    # 3. Stopwords removal\n",
        "    print(\"# 3. Stopwords removal:\")\n",
        "    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n",
        "    terms_df = stopwords_remover.transform(tokens_df).cache()\n",
        "    terms_df.show(10)\n",
        "\n",
        "    # 4. Stemming (Snowball stemmer)\n",
        "    print(\"# 4. Stemming:\")\n",
        "    stemmer = SnowballStemmer(language=\"english\")\n",
        "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))\n",
        "    terms_stemmed_df.show(10)\n",
        "    \n",
        "    return terms_stemmed_df.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOITbChE0WRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_news_df = clean_text(news_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYII3XAqCp7",
        "colab_type": "text"
      },
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "Machine learning techniques cannot work directly on text data; in fact, words must be first converted into some numerical representation which machine learning algorithms can make use of. This process is often known as _embedding_ or _vectorization_.\n",
        "\n",
        "In terms of vectorization, it is important to remember that it isn't merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Moreover, vectors derived from text data are usually high-dimensional. This is because each dimension of the feature space will correspond to a word, and the language in the documents may have thousands of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Ck6WXpAaqs",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF\n",
        "In information retrieval, **tf-idf** - short for term frequency-inverse document frequency - is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13RW7AdQBeD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_tfidf_features(df, column_name=\"terms_stemmed\"):\n",
        "    \"\"\" \n",
        "    This fucntion takes the text data and converts it into a term frequency-inverse document frequency vector\n",
        "\n",
        "    parameter: dataframe\n",
        "    returns: dataframe with tf-idf vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing the feature transformation classes for doing TF-IDF \n",
        "    from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "    from pyspark.ml import Pipeline\n",
        "\n",
        "    ## Creating Term Frequency Vector for each word\n",
        "    #cv = CountVectorizer(inputCol=column_name, outputCol=\"tf_features\", vocabSize=2000, minDF=5)\n",
        "    #cvModel = cv.fit(df)\n",
        "    #tf_features_df = cvModel.transform(df).cache()\n",
        "\n",
        "    ## Alternatively to CountVectorizer, use HashingTF\n",
        "    #hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=2000)\n",
        "    #tf_features_df = hashingTF.transform(df).cache()\n",
        "\n",
        "    ## Carrying out Inverse Document Frequency on the TF data\n",
        "    #idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "    #idfModel = idf.fit(tf_features_df)\n",
        "    #tf_idf_features_df = idfModel.transform(tf_features_df).cache()\n",
        "\n",
        "    # USING PIPELINE\n",
        "    cv = CountVectorizer(inputCol=column_name, outputCol=\"tf_features\", vocabSize=2000, minDF=10)\n",
        "    # hashingTF = HashingTF(inputCol=column_name, outputCol=\"tf_features\", numFeatures=2000)\n",
        "    idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[cv, idf]) # replace `cv` with `hashingTF` if needed\n",
        "    features = pipeline.fit(df)\n",
        "    tf_idf_features_df = features.transform(df).cache()\n",
        "\n",
        "    return tf_idf_features_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQprR3DjdYDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_df = extract_tfidf_features(clean_news_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcjFWSitdday",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_df.select(col(\"features\")).show(10, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_AVPZudEfZv",
        "colab_type": "text"
      },
      "source": [
        "### Check and remove any possible zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gus3IFnA7JAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@udf(\"long\")\n",
        "def num_nonzeros(v):\n",
        "    return v.numNonzeros()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFYhRWY_PwHC",
        "colab_type": "text"
      },
      "source": [
        "#### Check if there is any zero-lenght vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZj5ONJPuth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total n. of zero-length vectors: {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23FneXsvQJaZ",
        "colab_type": "text"
      },
      "source": [
        "#### Remove zero-lenght vector(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-YCkcO4PrWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_df = tf_idf_df.where(num_nonzeros(\"features\") > 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEAtctNbQNeG",
        "colab_type": "text"
      },
      "source": [
        "#### Double-check there is no more zero-length vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TNJ35M0GoBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total n. of zero-length vectors (after removal): {:d}\".\n",
        "      format(tf_idf_df.where(num_nonzeros(\"features\") == 0).count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2DTA2NdfSA",
        "colab_type": "text"
      },
      "source": [
        "# **K-means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPdq2yVE9LzC",
        "colab_type": "text"
      },
      "source": [
        "### Function used for running K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1deqVUurtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_means(dataset, \n",
        "            n_clusters, \n",
        "            distance_measure=\"euclidean\", \n",
        "            max_iter=20, \n",
        "            features_col=\"features\", \n",
        "            prediction_col=\"cluster\", \n",
        "            random_seed=RANDOM_SEED):\n",
        "  \n",
        "  from pyspark.ml.clustering import KMeans\n",
        "\n",
        "  print(\"\"\"Training K-means clustering using the following parameters: \n",
        "  - K (n. of clusters) = {:d}\n",
        "  - max_iter (max n. of iterations) = {:d}\n",
        "  - distance measure = {:s}\n",
        "  - random seed = {:d}\n",
        "  \"\"\".format(n_clusters, max_iter, distance_measure, random_seed))\n",
        "  # Train a K-means model\n",
        "  kmeans = KMeans(featuresCol=features_col, \n",
        "                   predictionCol=prediction_col, \n",
        "                   k=n_clusters, \n",
        "                   initMode=\"k-means||\", \n",
        "                   initSteps=5, \n",
        "                   tol=0.000001, \n",
        "                   maxIter=max_iter, \n",
        "                   seed=random_seed, \n",
        "                   distanceMeasure=distance_measure)\n",
        "  model = kmeans.fit(dataset)\n",
        "\n",
        "  # Make clusters\n",
        "  clusters_df = model.transform(dataset).cache()\n",
        "\n",
        "  return model, clusters_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3FUnyx9ZWi",
        "colab_type": "text"
      },
      "source": [
        "### Function used to evaluate obtained clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZTOi7TWvcXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_k_means(clusters, \n",
        "                     metric_name=\"silhouette\", \n",
        "                     distance_measure=\"squaredEuclidean\", # cosine\n",
        "                     prediction_col=\"cluster\"\n",
        "                     ):\n",
        "  \n",
        "  from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "  \n",
        "  # Evaluate clustering by computing Silhouette score\n",
        "  evaluator = ClusteringEvaluator(metricName=metric_name,\n",
        "                                  distanceMeasure=distance_measure, \n",
        "                                  predictionCol=prediction_col\n",
        "                                  )\n",
        "\n",
        "  return evaluator.evaluate(clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1p5kISG9et-",
        "colab_type": "text"
      },
      "source": [
        "### Run K-means by calling the function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgvwHoeBvmzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, clusters_df = k_means(tf_idf_df, 10, max_iter=100, distance_measure=\"euclidean\") # Alternatively, distance_measure=\"cosine\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYu_vheKwTc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_k_means(clusters_df, distance_measure=\"squaredEuclidean\") # Alternatively, distance_measure=\"cosine\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1-52c-HtVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters_df.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2MLijHVJHIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters_df.groupBy(\"cluster\").count().sort(\"cluster\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqft7DRcHUoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get unique values in the grouping column\n",
        "clusters = sorted([x[0] for x in clusters_df.select(\"cluster\").distinct().collect()])\n",
        "print(\"Cluster IDs: [{:s}]\".format(\", \".join([str(c) for c in clusters])))\n",
        "\n",
        "# Create a filtered DataFrame for each group in a list comprehension\n",
        "cluster_list = [clusters_df.where(clusters_df.cluster == x) for x in clusters]\n",
        "\n",
        "# Show the results\n",
        "for x_id, x in enumerate(cluster_list):\n",
        "  print(\"Showing the first 10 records of cluster ID #{:d}\".format(x_id))\n",
        "  x.select([\"cluster\", \"id\", \"content\"]).show(10, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}